\documentclass[8pt,oneside]{extarticle}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[a4paper,left=2.3cm,right=1.2cm,top=2cm,bottom=2cm]{geometry} 
%\usepackage{blindtext}
\usepackage{setspace}
%\usepackage{float}
\usepackage{titletoc}
\usepackage{titlesec}
%\usepackage{wrapfig}
%\usepackage{tikz}
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{amsfonts} 
\usepackage{comment}
%\usepackage{booktabs}
\usepackage{bbm}
%\usepackage{wrapfig}
%\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}
%\usepackage{bigints}
\onehalfspacing
\usepackage[hidelinks]{hyperref}
\usepackage[round, sectionbib]{natbib} 
\usepackage{tcolorbox}

%\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}




%\allowdisplaybreaks

\setlength\parindent{0pt}

\newcommand{\zerodisplayskips}{%
  \setlength{\abovedisplayskip}{2pt}%
  \setlength{\belowdisplayskip}{2pt}%
  \setlength{\abovedisplayshortskip}{2pt}%
  \setlength{\belowdisplayshortskip}{2pt}}
\appto{\normalsize}{\zerodisplayskips}
\appto{\small}{\zerodisplayskips}
\appto{\footnotesize}{\zerodisplayskips}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\indep}{\perp \!\!\! \perp}

%Hier sind die unterschiedlichen Ausführlichkeitsgrade definiert
\includecomment{Extensiv} 
\includecomment{Proof} 
\includecomment{Annahmen}
\includecomment{Mathspez}
\includecomment{Mathfolg}
\includecomment{Rechreg}
\mdfdefinestyle{MyFrame}{%
    linecolor=black!20!,
    outerlinewidth=0.2pt,
    roundcorner=5pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
    backgroundcolor=white}
\specialcomment{Proof}{\begin{mdframed}[style=MyFrame,nobreak=false]  }{\end{mdframed}}
\specialcomment{Rechreg}{\noindent \textit{Calculation Rules:} \begin{itemize}[nosep,label=$\star$] }{\end{itemize}}
\renewcommand\ThisComment[1]{% Fix for Umlauts in comments
  \immediate\write\CommentStream{\unexpanded{#1}}%
}

% Hier die Ausführlichkeit bestimmen:
%\excludecomment{Extensiv} 
%\excludecomment{Proof} 
%\excludecomment{Annahmen}
%\excludecomment{Mathspez}
%\excludecomment{Mathfolg}

% Inhaltsverzeichnis mit zwei Spalten
\usepackage[toc]{multitoc}
\renewcommand*{\multicolumntoc}{2}




%Überschriftengrößen anpassen, so dass Paragraph kleiner ist als Subsubsection
\titleformat{\section}
  {\normalfont\fontsize{16}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesubsubsection}{1em}{}


\begin{document}

%\topskip0pt
\vspace*{18em}

\hrule
\begin{center}
{\fontsize{30}{60}\selectfont \textbf{Causal Inference}} \\ \

{\fontsize{20}{60}\selectfont a summary}
\end{center}
\hrule



\tableofcontents



% weitere Anpassungen im Hauptteil des Dokuments
\raggedright %linksbündig
\setlength{\parindent}{15pt} %Einzuglänge festsetzen
\setlength{\columnseprule}{0.3pt} %Liniendicke zwischen zwei Multicols






%-------------------------------------------------------------------------------

% SECTION: GENERAL

%-------------------------------------------------------------------------------

\section{General}

\begin{multicols}{2}

\paragraph{\large Causal Roadmap} \citep{petersen2014causal} 
systematic approach linking causality to statistical procedures

\noindent \textbf{1. Specifying Knowledge.} structural causal model (unifying counterfactual language, structural equations, \& causal graphs): a set of possible data-generating processes, expresses background knowledge and its limits

\noindent \textbf{2. Linking Data.} specifying measured variables and sampling specifics (latter can be incorporated into the model)

\noindent \textbf{3. Specifying Target.} define hypothetical experiment: decide
\noindent\begin{enumerate}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item variables to intervene on: one (point treatment), multiple (longitudinal, censoring/missing, (in)direct effects)
\item intervention scheme: static, dynamic, stochastic
\item counterfactual summary of interest: absolute or relative, marginal structural models, interaction, effect modification
\item population of interest: whole, subset, different population
\end{enumerate}

\noindent \textbf{4. Assessing Identifiability.} are knowledge and data sufficient to derive estimand and if not, what else is needed?

\noindent \textbf{5. Select Estimand.} current best answer: knowledge-based assumptions $+$ which minimal convenience-based asspumptions (transparency) gets as close as possible

\noindent \textbf{6. Estimate.} choose estimator by statistical properties, nothing causal here

\noindent \textbf{7. Interpret.} hierarchy: statistical, counterfactual, feasible intervention, randomized trial


\paragraph{\large Average Causal Effect} $\mathrm{E}\left[Y^{a=1}\right] \neq \mathrm{E}\left[Y^{a=0}\right]$
\begin{align*}
\mathrm{E}\left[Y^{a}\right]   &=  \sum_yyp_{Y^a}(y)  &\text{(discrete)}\\
   &=  \int yf_{Y^a}(y)dy &\text{(continuous)}
\end{align*}

\noindent individual causal effect $Y_i^{a=1} \neq Y_i^{a=0}$ generally unidentifiable

\noindent \textit{null hypothesis:} no average causal effect

\noindent \textit{sharp null hypothesis:} no causal effect for any individual

\noindent \textbf{notation} $A,Y$: random variables (differ for individuals);
$a,y$: particular values; counterfactual $Y^{a=1}$: $Y$ under treatment $a=1$

\noindent  \textbf{stable unit treatment value assumption (SUTVA)} $Y_i^a$ is well-defined: no interference between individuals, no multiple versions of treatment (weaker: treatment variation irrelevance)

\noindent  \textbf{causal effect measures} typically based on means

\textit{risk difference:}  $\mathrm{Pr}\left[Y^{a=1}=1\right] - \mathrm{Pr}\left[Y^{a=0}=1\right]$

\textit{risk ratio:} $\frac{\mathrm{Pr}\left[Y^{a=1}=1\right]}{ \mathrm{Pr}\left[Y^{a=0}=1\right]}$

\textit{odds ratio:} $\frac{\mathrm{Pr}\left[Y^{a=1}=1\right]/\mathrm{Pr}\left[Y^{a=1}=0\right]}{ \mathrm{Pr}\left[Y^{a=0}=1\right]/\mathrm{Pr}\left[Y^{a=0}=0\right]}$

\noindent \textit{number needed to treat (NNT)} to save 1 life: $-1/$risk difference

\noindent \textbf{sources of random error}: sampling variability (use consistent estimators), nondeterministic counterfactuals

\noindent\textbf{association} compares $E\left[Y|A=1\right]$ and $E\left[Y|A=0\right]$, \textbf{causation} compares $E\left[Y^{a=1}\right]$ and $E\left[Y^{a=0}\right]$ (whole population)









\paragraph{\large Target Trial} emulating an ideal randomized experiment

\noindent explicitly formulate target trial \& show how it is emulated $\rightarrow$ \newline less vague causal question, helps spot issues

\noindent \textbf{missing data problem} unknown counterfactuals

\noindent \textit{randomized experiments:} missing completely at random $\rightarrow$ exchangeability (= exogeneity as treatment is exogenous)

\noindent \textit{ideal randomized experiment:} no censoring, double-blind, well-defined treatment, \& adherence $\rightarrow$ association is causation

\noindent \textit{pragmatic trial:} no placebo/blindness, realistic monitoring


\noindent \textbf{PICO} (population, intervention, comparator, outcome): some components of target trial

\noindent \textbf{three types of causal effects:}

\textit{intention-to-treat effect} (effect of treatment assignment)

\textit{per-protocol effect} (usually dynamic when toxicity arises)

\textit{other intervention effect} (strategy changed during follow-up)


\noindent \textbf{controlled direct effects:} effect of A on Y not through B

\textit{natural direct effect}  $A$ on $Y$ if $B^{a=0}$ (cross-world quantity)

\textit{principal stratum effect} $A$ on $Y$ for subset with $B^{a=0} = B^{a=1}$

\noindent \textbf{crossover experiment:} sequential treatment \& outcome $t{=}0,1$
\newline
individual causal effect $Y_{it}^{a_t=1} - Y_{it}^{a_t=0}$ only identifiable if: no carryover effect, effect $\indep$ time, outcome $\indep$ time

\noindent \textbf{time zero} if eligibility at multiple $t$ (observational data):
earliest, random $t$, all $t$ (adjust variance with bootstrapping)

\noindent \textbf{grace periods:} usually treatment starts $x$ months after first eligible, if death before: randomly assign strategy/copy into both




\paragraph{\large Identifiability Conditions} hold in ideal experiments

\noindent \textbf{consistency} counterfactuals correspond to data $Y=Y^A$: \newline
 if $A=a$, then $Y^a=Y$ for each individual
 
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item precise definition of $Y^a$ via specifying $a$ (sufficiently well-defined $a$ maybe impossible (effect of DNA before it was discovered), relies on expert consensus)
\item linkage of counterfactuals to data ($a$ must be seen in data) 
\end{itemize}

\noindent \textbf{positivity} $\mathrm{Pr}\left[A=a|L=l\right] >0 \,\,\, \forall \, l \text{ with } \mathrm{Pr}\left[L=l\right]>0$; $$f_L(l)\neq 0 \Rightarrow f_{A|L}(a|l)>0 \,\, \forall a,l$$

\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item structural violations (inference not on full population)
\item random variability (smooth over with parametric models)
\end{itemize}
\noindent can sometimes be empirically verified (if all is seen in data)

\noindent \textbf{exchangeability} unverifiable without randomization
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item \textit{marginal:} $Y^a \indep A$ $\widehat{=}$ randomized experiment, \newline counterfactuals are missing completely at random (MCAR)

\item \textit{conditional:} $Y^a {\indep} A|L$ $\widehat{=}$ conditionally randomized, counterfactuals are missing at random (MAR)
\end{itemize}
alternative definition: $ \mathrm{Pr}\left[A=1|Y^{a=0}, L\right] = \mathrm{Pr}\left[A=1|L\right]$

\noindent \textbf{additional conditions:}

\noindent \textit{correct measurement} mismeasurement of $A, Y, L$ results in bias

\noindent \textit{correct model specification} models $\overset{\text{may}}{\rightarrow}$ misspecification bias




\paragraph{\large Effect Modification} $A$ on $Y$ varies across levels of $V$

\noindent null average causal effect $\neq$ null causal effect per subgroup

\noindent \textbf{population characteristics:} causal effect measure is actually ``effect in a population with a particular mix of effect modifiers''

\noindent \textbf{transportability:} extrapolation of effect to another population (issues: effect modification, versions of treatment, interference)

\noindent effects conditional on $V$ may be more transportable

\noindent \textbf{types:} additive/multiplicative scale,
qualitative (effect in opposite directions)/quantitative, surrogate/causal 


\noindent \textbf{calculation:} 
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item \textit{stratify} by $V$ then standardize/IP weight for $L$, 
\item $L$ as \textit{matching} factor (ensures positivity, difficult if high-dimensional $L$)
\end{itemize}

\noindent \textbf{collapsibility:}  causal risk difference and ratio are weighted averages of stratum-specific risks, can not be done for odds ratio



\paragraph{\large Interaction} effects of joint interventions $A$ and $E$ 
$$\mathrm{Pr}\left[Y^{1,1}{=}1\right] - \mathrm{Pr}\left[Y^{0,1}{=}1\right] \neq \mathrm{Pr}\left[Y^{1,0}{=}1\right] - \mathrm{Pr}\left[Y^{0,0}{=}1\right]$$
$A$ and $E$ have equal status and could also be considered a combined treatment $AE$, exchangeability for both is needed
\textit{additive scale} (above): ``$>$'' superadditive and ``$<$'' subadditive;
\textit{multiplicative scale:} ``$>$'' super- and ``$<$'' submultiplicative

\noindent \textbf{difference to effect modification:} if $E$ is randomly assigned methods coincide, but $V$ can not be intervened on as $E$ can 

\noindent \textbf{monotonicity} effect is either nonnegative or nonpositive $\forall i$

\noindent \textbf{sufficient component-cause framework} pedagogic model

\noindent \textit{response types} for binary $A$: helped, immune, hurt, doomed;

\noindent for binary $A$ and $E$: 16 types

\noindent \textit{(minimal) sufficient causes: }
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item (minimal) $U_1$ together with $A=1$ ensure $Y=1$
\item (minimal) $U_2$ together with $A=0$ ensure $Y=1$
\end{itemize}
\textit{sufficient cause interaction:} $A$ and $E$ appear together in a minimal sufficient cause


\paragraph{\large NPSEM} \textit{n}on\textit{p}aramentric \textit{s}tructural \textit{e}quation \textit{m}odel
$$V_m = f_m(pa_m, \epsilon_m)$$

\noindent counterfactuals are obtained recursively, e.\,g.\  $V_3^{v_1} = V_3^{v_1, V_2^{v_1}}$

\noindent implies any variable can be intervened on

\noindent aka finest causally interpreted structural treee graph (FCISTG)

\noindent \textbf{additional assumption} $\cap$ FCISTG $\Rightarrow$ causal Markov condition:
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item independent errors (NPSEM-IE): all $\epsilon_m$ mutually independent
\item fully randomized (FFRCISTG):  $V_m^{\bar{v}_{m-1}} \indep V_j^{\bar{v}_{j-1}}$ if $\bar{v}_{j-1}$ subvector of $\bar{v}_{m-1}$
\end{itemize}

\noindent NPSEM-IE $\Rightarrow$ FFRCISTG (assume DAGs represent latter)

\noindent NPSEM-IE assume crossworld independencies $\to$ unverifiable


\paragraph{\large Causal DAG} draw assumptions before conclusions


\noindent \textit{rules:}
arrow means direct causal effect for at least one $i$, absence means sharp null holds, all common causes are on the graph


\noindent \textit{neglects:} direction of cause (harmful/protective), interactions

\noindent \textit{convention:} time flows from left to right


\noindent \textbf{causal Markov assumption:} any variable ($v$) $|$ its direct causes ($pa_j$) $\indep$ its non-descendants ($\lnot v_j$) $\Leftrightarrow$ Markov factorization $$f(v) = \textstyle\prod_{j=1}^Mf(v_j|pa_j)$$
\noindent \textbf{d-separation} (d for directional): a pathway in a DAG is ...
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item blocked if collider or conditioned on non-collider
\item opened if conditioned on collider or descendent of collider
\end{itemize}
2 variables are d-separated if all connecting paths are blocked

\noindent under causal Markov: d-separation $\Rightarrow$ independence

\noindent under faithfulness: independence $\Rightarrow$ d-separation

\noindent \textbf{faithfulness:} effects don't cancel out perfectly

\noindent \textit{discovery:} process of learning the causal structure; requires faithfulness, but even with it is often impossible







\paragraph{\large SWIGs} \textit{s}ingle \textit{w}orld \textit{i}ntervention \textit{g}raphs

\noindent \textbf{counterfactual graphic approach:}
$A$ turns into $A|a$, the left (right) side inherits incoming (outgoing) arrows (intervention with $A=a$); all outcomes of $A$ get a superscript $a$, e.\,g. $Y^a$; more than one intervention possible, dynamic strategies require additional arrows from $L$ to $a$


\noindent $A$ and $Y^a$ are d-separated $\rightarrow$ $Y^a \indep A|L$ (for FFRCISTG)



\paragraph{\large Confounding} bias due to common cause of $A$ \& $Y$ \textbf{\textit{not in}} $L$

\noindent randomization prevents confounding

\noindent \textbf{backdoor path:} noncausal path $A$ to $Y$ with arrow into $A$

\noindent \textbf{backdoor criterion:} all backdoor paths are blocked by $L$ \& no descendants of $A$ in $L$ $\Rightarrow$ conditional exchangeability

\noindent $Y^a {\indep} A|L \Rightarrow$ $L$ fulfills backdoor criterion if faithful (FFRCISTG)



\noindent \textbf{confounders in observational studies:} occupational factors \textit{(healthy worker bias),} clinical decisions \textit{(confounding by indication/channeling),} lifestyle, genetic factors \textit{(population stratification),} social factors, environmental exposures

\noindent given a DAG, confounding is an absolute, confounder is relative

\noindent surrogate confounders in $L$ may reduce confounding bias

\noindent \textbf{negative outcome controls:} if $A$ and $Y$ share a common cause $U$: measure effect for $Y_0$ (before treatment) and $Y_1$ (after), subtract (assumption of additive equi-confounding)

\noindent \textbf{front door criterion} using the full mediator $M$: $\mathrm{Pr}\left[Y^a=1\right]=$
$$\sum_m \mathrm{Pr}\left[M=m|A=a\right] \sum_{a'} \mathrm{Pr}\left[Y=1|M=m, A=a'\right]\mathrm{Pr}\left[A=a'\right]$$


\paragraph{\large Selection Bias} bias due to common effect of $A$ \& $Y$ \textbf{\textit{in}} $L$

 \noindent $=$ conditioning on collider (can't be fixed by randomization)


\noindent \textbf{examples:} informative censoring, nonresponse bias, healthy worker bias, volunteer bias; often M-bias ($A {\leftarrow} U_1 {\to} L {\leftarrow} U_2 {\to} Y$)


\noindent \textbf{solution:} target $Y^{A, C}$, $AC$ fulfills identifiability conditions, \newline
 if competing events, interventions may not be well-defined


\noindent \textbf{multiplicative survival model:} $\mathrm{Pr}\left[Y{=}0|E{=}e, A{=}a\right]{=}g(e)h(a)$
$\rightarrow$ no interaction between E and A on the multiplicative scale; \newline
if $Y=0$ is conditionally independent, then $Y=1$ can't be as $\mathrm{Pr}\left[Y{=}1|E{=}e, A{=}a\right]{=}1-g(e)h(a)$
$\rightarrow$ conditioning on a collider could be unbiased if restricted to certain levels ($Y=0$)


\paragraph{\large Measurement Bias} aka information bias 

\noindent measurements $X^*$ of variables $X$ can be included in DAG

\noindent \textbf{independent} errors $U$ if $f(U_A, U_Y) = f(U_A)f(U_Y)$

\noindent \textbf{nondifferential} $A$:  if $f(U_A|Y)=f(U_A)$; $Y$: $f(U_Y|A)=f(U_Y)$

\noindent mismeasurement $\to$ bias, if: $A\to Y$ \textit{or} dependent \textit{or} differantial

\noindent \textbf{reverse causation bias} caused by e.\,g.\ recall bias: independent but differential $A$ (caused by $Y \to U_A$)


\noindent \textbf{misclassified treatment:} assignment $Z$ does not determine $A$

\noindent \textit{exclusion restriction:} ensure  $Z\not\to Y$, e.\,g.\ via double-blinding



\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item \textit{\textbf{per-protocol effect:}} either as-treated ($\to$ confounded) or restricted to protocol adhering individuals ($\to$ selection bias)
\item \textit{\textbf{intention-to-treat effect}} ($\to$ measurement bias): advantages: $Z$ is randomized, preserves null (if exclusion restriction holds), $=$ underpowered $\alpha$-level test of the null (only if monotonicity; underpowered may be problematic if treatment safety is tested)
\end{itemize}





\paragraph{\large Random Variabilty}  quantify uncertainty due to small $n$

\noindent \textbf{CI}: e.\,g.\ Wald CI $=\hat{\theta} \pm 1.96 \times se(\hat{\theta})$, \textit{calibrated} if it contains 95\,\% of estimands ($>$: \textit{conservative}, $<$: \textit{anticonservative})

\noindent \textit{large sample} CI: converge to 95\,\% vs.\ \textit{small-sample:} always valid



\noindent \textit{honest:}\ $\exists n$ where coverage $\geq 95\,\%$,
\textit{valid:}\ large-sample \& honest

\noindent \textbf{inference:} either
restrict inference to sample (randomization- based inference) or inference on super-population


\noindent \textbf{super-population:} generally a fiction, but $\to$ simple statistical properties (where does the variability of the distribution come from: 
assumption population is sampled from super-population)

\noindent \textbf{conditionality principle:} inference should be performed conditional on ancillary statistics (e.g. L-A association) as $$\mathcal{L}(Y)=f(Y|A, L)f(A|L)f(L)$$

\noindent\textit{exactly ancillary} $A,L$: $f(Y|A, L)$ depends on parameter of interest, but $f(A,L)$ does not share parameters with $f(Y|A, L)$


\noindent\textit{approximately ancillary:}  ... does not share \textit{\textbf{all}} parameters ... 
continuity principle: also condition on approximate ancillaries

\noindent \textbf{curse of dimensionality:} difficult to do conditionality principle




\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: MODELS

%-------------------------------------------------------------------------------

\section{Models}

\begin{multicols}{2}

\paragraph{\large Modeling} data are a sample from the target population \vspace{0.4em}

\noindent \hspace{0.9em}\begin{tabular}{l l l}
\textbf{\it estimand:} & quantity  of interest, & e.\,g.\ $\mathrm{E}\left[Y|A=a\right]$ \\
\textbf{\it estimator:} & function to use, & e.\,g.\ $\widehat{\mathrm{E}}\left[Y|A=a\right]$ \\
\textbf{\it estimate:} & apply function to data, & e.\,g.\ $4.1$ 
\end{tabular} \vspace{0.5em}

\noindent \textbf{model}: a priori restriction of joint distribution/dose-response curve;  \textit{assumption:} no model misspecification (usually wrong)

\noindent \textbf{non-parametric estimator:} no restriction (saturated model) $=$ \textit{Fisher consistent estimator} (entire population data $\rightarrow$ true value)

\noindent \textbf{parsimonious model:} few parameters estimate many quantities

\noindent \textbf{bias-variance trade-off:} \newline wiggliness $\uparrow$ $\rightarrow$ misspecification bias $\downarrow$, CI width $\uparrow$

\paragraph{\large Variable Selection} can induce bias if $L$ includes: 

\hspace{-0.2em}\vspace{-1em}
\begin{tabular}{l l }
 (decendant of) collider:& \textit{selection bias under the null}\\
 noncollider effect of $A$:& \textit{selection bias under the alternative}\\
 mediator:& \textit{overadjustment for mediators}
\end{tabular}

\noindent temporal ordering is not enough to conclude anything

\noindent \textbf{bias amplification:} e.g. by adjusting for an instrument $Z$ (can also reduce bias)





\paragraph{\large Machine Learning} $L$ is high-dimensional

\noindent use lasso or ML for IP weighting/standardization

\noindent \textbf{\textit{but:}} ML does not guarantee elimination of confounding and has largely unknown statistical properties

\noindent $\rightarrow$ \textbf{doubly robust estimator:} consistent if bias  $< \frac{1}{\sqrt{n}}$

\noindent \textit{sample splitting:} train estimators on training sample, use resulting estimators for doubly robust method on estimation sample (CIs on estimation sample are valid, but $n$ halved)

\noindent \textit{cross-fitting:} do again the other way round, average the two estimates, get CI via bootstrapping

\noindent \textbf{problems:} unclear choice of algorithm, is bias small enough?



\paragraph{\large Super Learning} \citep{van2007super, van2011targeted}

\noindent \textbf{oracle selector:} select best estimator of set of learners $Z_i$

\noindent \textbf{discrete super learner:} select algorithm with smallest cross-validated error (converges to oracle for large sample size)

\noindent \textbf{super learner:} improves asymptotically on discrete version

$\mathrm{logit} (Y=1|Z) = \sum_i \alpha_i Z_i, $ with $0<\alpha_i<1$ and $\sum\alpha_i=1$
weights $\alpha_i$ are determined inside the cross-validation; for the prediction, $Z_i$ trained on the full data set are used

\noindent   can be cross-validated itself to check for overfitting (unlikely)












\end{multicols}

\subsection{Traditional Methods}

\begin{multicols}{2}




\paragraph{\large Stratification}  calculate risk for each stratum of $L$

\noindent only feasible if enough data per stratum


\paragraph{\large Outcome Regression} often assume no effect modification
$$\mathrm{E}\left[Y^{a,c=0}|L\right] = \beta_0 + \beta_1 a + \beta_2 aL +\beta_3 L = \mathrm{E}\left[Y|A, C=0, L\right]$$

\noindent faux marginal structural model as no IP weighting/$SW^A(L)=1$

\noindent for ATE only $\beta_1,\beta_2$ of interest, the rest are \textit{nuisance parameters}

\paragraph{\large Propensity Score Methods}
$\mathrm{Pr}\left[A=1|L\right] =: \pi(L)$ 
 
\noindent  $\Rightarrow A\indep L|\pi(L)$ (definition of a  balancing score); can be modelled  


\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item \textit{\textbf{stratification:}} create strata with similar $\pi(L)$ (e.\,g.\ deciles), but the average $\pi(L)$ might still be different in some strata
\item \textit{\textbf{standardization:}} use $\pi(L)$ instead of $L$ to standardize
\item \textit{\textbf{matching:}} find close ($\rightarrow$ bias-variance trade-off) values of $\pi(L)$, positivity issues arise often
\end{itemize}

\noindent propensity models don't need to predict well, just ensure exchangeability (good prediction leads to positivity problems)


\paragraph{\large Instrumental Variable Estimation} $L$ unmeasured

\noindent surrogate/proxy instruments can be used

\noindent \textbf{instrumental conditions:} 
\begin{enumerate}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item relevance condition: $Z \not\!\indep A$ ($Z$ is associated with $A$), weak association $\rightarrow$ weak instrument
\item exclusion restriction: $Z$ affects $Y$ at most through $A$
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item population level: $\mathrm{E}\left[Y^{z,a}\right] = \mathrm{E}\left[Y^{z',a}\right]$ (sometimes enough)
\item individual level: $Y^{z,a}_i =  Y^{z',a}_i = Y^{a}_i$
\end{itemize}
\item exchangeability: $Z$ and $Y$ have no shared causes
\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item marginal: $Y^{a,z} \indep Z$ (typically enough)
\item joint: $\left\{Y^{z, a};a\in\left[0,1\right],z\in\left[0,1\right]\right\} \indep Z$
\end{itemize}
\end{enumerate}



\noindent \textbf{common instruments:} (physician's) general preference, access to/price of $A$, genetic factors (Mendelian randomization)

%todo bounds

%todo everything from 16.2 onwards












\paragraph{causal survival analysis} chapter 17 (and technical point 22.3)









\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: G-METHODS

%-------------------------------------------------------------------------------

\subsection{G-Methods}
\begin{multicols}{2}


\paragraph{\large G-Methods} \textit{g}eneralized treatment contrasts:
adjust for (surrogate) confounders $L$




\begin{itemize}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item \textbf{standardization} two types of g-formula
\item \textbf{IP weighting} also g-formula
\item \textbf{g-estimation:} not needed unless longitudinal
\end{itemize}

\paragraph{\large Standardization} plug-in (or parametric if so) g-formula
$$\mathrm{E}\left[Y^{a}\right] = \overbrace{\mathrm{E}\left[\mathrm{E}\left[Y|A{=}a,L{=}l\right]\right]}^{\text{conditional expectation}} =  \overbrace{ \textstyle{\int} \mathrm{E}\left[Y|L=l, A=a\right]f_L\left[l\right]dl}^{\text{joint density estimator}} $$
weighted average of stratum-specific risks; unknowns can be estimated non-parametrically or modeled

\noindent \textbf{no need to estimate $\boldsymbol{f_L\left[l\right]}$/integrate} as empirical distribution can be used: estimate outcome model $\rightarrow$ predict counterfactuals on whole dataset $\rightarrow$ average the results ($\rightarrow$ CI by bootstrapping)

\noindent \textbf{for discrete $\boldsymbol{ L}$}  $\mathrm{E}\left[Y|A=a\right] = \sum_l \mathrm{E}\left[Y|L=l, A=a\right]\mathrm{Pr}\left[L=l\right]$

\vspace{0.2em}
\noindent \colorbox{lightgray!20!white}{\begin{minipage}{28em}




\textbf{\textcolor{darkgray}{time-varying}} standardize over all possible $\bar{l}$-histories



\noindent simulates joint distribution of counterfactuals $\left(Y^{\bar{a}}, \bar{L}^{\bar{a}}\right)$ for $\bar{a}$
\textbf{joint density estimator (jde)}
\begin{align*}
& \text{discrete: } \mathrm{E}\left[Y^{\bar{a}}\right] = \sum_{\bar{l}} \mathrm{E}\left[Y|\bar{A}=\bar{a}, \bar{L}=\bar{l} \right] \prod_{k=0}^K f \left(l_k|\bar{a}_{k-1}, \bar{l}_{k-1}\right)  \\ &
\text{continuous: } \int f(y|\bar{a}, \bar{l}) \prod_{k=0}^K f\left(l_k|\bar{a}_{k-1}, \bar{l}_{k-1}\right)dl
\end{align*}

\noindent for \textit{stochastic strategies} multiply with $\prod_{k=0}^K f^{int} \left(a_k|\bar{a}_{k-1}, \bar{l}_{k}\right) $


\begin{mdframed}[linecolor=black!20!,
    outerlinewidth=0.2pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    backgroundcolor=lightgray!20!white, innerleftmargin=2pt, innerrightmargin=2pt]
\textbf{estimation} \citep{young2011comparative, schomaker_using_2019}

\begin{enumerate}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item model $f \left(l_k|\bar{a}_{k-1}, \bar{l}_{k-1}\right)$ and $\mathrm{E}\left[Y|\bar{A}=\bar{a}, \bar{L}=\bar{l} \right]$
\item simulate data forward in time: \newline
at $k=0$: use empirical distribution of $L_0$ (observed data) \newline
at $k>0$: set $\bar{A} = \bar{a}$, \textit{draw} from models estimated in 1.
\item calculate mean of $\hat{Y}_{K,i}^{\bar{a}}$ (bootstrap for CI)
\end{enumerate}



\end{mdframed}


\textbf{iterated conditional expectation (ice)} 
$$\mathrm{E}\left[ Y_T^{\bar{a}}\right] = \mathrm{E} \left[ \mathrm{E} \left[\mathrm{E} \left[ ... \mathrm{E} \left[Y_T| \bar{A}_{T{-}1} {=} \bar{a}_{T{-}1}, \bar{L}_T \right]  ...|\bar{A}_0 {=} a_0, L_1 \right] |L_0 \right]\right]$$

%\end{minipage}}

%\noindent \colorbox{lightgray!20!white}{
%\begin{minipage}{28em}


\begin{mdframed}[linecolor=black!20!,
    outerlinewidth=0.2pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    backgroundcolor=lightgray!20!white, innerleftmargin=2pt, innerrightmargin=2pt]
\textbf{estimation} \citep{schomaker_using_2019}

\begin{enumerate}[itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt, leftmargin=1.5em]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item model inside out: $Q_{T} {=} \mathrm{E} \left[Y_T| \bar{A}_{T{-}1}, \bar{L}_T \right]$ to $Q_0 {=} \mathrm{E} \left[Q_1| \bar{L}_0 \right]$, predict $Q_t$ with $\bar{A} = \bar{a}$ in each step
\item calculate mean of $\hat{Q}_{0,i}^{\bar{a}}$ (bootstrap for CI)
\end{enumerate}



\end{mdframed}

\noindent \textbf{g-null paradox} even if the sharp null holds, model misspecification can lead to it being falsely rejected

\end{minipage}}



\begin{mdframed}[style=MyFrame,nobreak=true, innerleftmargin=2pt, innerrightmargin=2pt]
Proof: for $L_0 \rightarrow A_0 \rightarrow Y_0 \rightarrow L_1 \rightarrow A_1 \rightarrow Y_1$, $\bar{a} =(a_0,a_1)$ 
\begin{alignat*}{4} 
\mathrm{E}\left[Y_1^{\bar{a}}\right] & \overset{\text{CE}}{=} && \mathrm{E}\left[\mathrm{E}\left[   Y_1^{\bar{a}}|A_0{=}a_0, L_0   \right]\right]  \\ 
\text{(ice)}\,\,\, &\overset{\text{CE*}}{=} && \mathrm{E}\left[\mathrm{E}\left[  \mathrm{E}\left[Y_1|\bar{L}, \bar{A}{=}\bar{a}, Y_0 \right]| A_0{=}a_0, L_0   \right]\right]  \\ 
&\overset{\text{LTP}}{=} && \mathrm{E}\left[ \sum\nolimits_{l_1} \mathrm{E}\left[Y_1|A_0{=}a_0, \bar{L}, Y_0\right] \mathrm{Pr}\left[l_1|a_0,l_0, y_0\right]         \right] \\
&\overset{\text{LTP}}{=} && \sum\nolimits_{l_0}\!\!\!\left[ \sum\nolimits_{l_1} \!\!\!\mathrm{E}\left[Y_1|A_0{=}a_0, \bar{L}, Y_0\right] \mathrm{Pr}\left[l_1|a_0,l_0, y_0\right]         \right] \mathrm{Pr}\left[l_0\right] \\
\text{(jde)}\,\,\, &\overset{\text{sum}}{=} &&  \sum\nolimits_{\bar{l}} \mathrm{E}\left[Y_1|A_0{=}a_0, \bar{L}, Y_0\right] \mathrm{Pr}\left[l_1|a_0,l_0\right]          \mathrm{Pr}\left[l_0\right] 
\end{alignat*}
CE: conditional expectation; *: exchangeability; \newline LTP: law of total probability
\end{mdframed}


















\paragraph{\large Marginal Structural Models} association is causation in the IP weighted pseudo-population 
$$\text{associational model } \mathrm{E}\left[Y|A\right] = \text{ causal model } \mathrm{E}\left[Y^a\right]$$

\noindent \textit{step 1:} estimate/model $f\left[A|L\right]$ (and $f\left[A\right]$) $\rightarrow$ get $(S)W^A$

\noindent \textit{step 2:} estimate regression parameters for pseudo-population

\noindent \textbf{effect modification} variables $V$ can be included (e.\,g.\ $\beta_0+\beta_1 a+\beta_2 V a + \beta_3 V$; technically not marginal anymore), $SW^A(V) = \frac{f\left[A|V\right]}{f\left[A|L\right]}$ more efficient than $SW^A$
 

\paragraph{\large Censoring} measuring joint effect of $A$ and $C$
$$\mathrm{E}\left[Y^{a, c=0}\right] \text{ is of interest}$$

\noindent \textbf{standardization} $\mathrm{E}\left[Y|A=a\right] = \int \mathrm{E}\left[Y|L{=}l, A{=}a, C{=}0\right]dF_L\left[l\right]$\vspace{0.2em}

\noindent \textbf{IP weights}\vspace{-1.30em}

\hspace{3.5em}\begin{tabular}{l l l}
$W^{A,C}=W^A \times W^C$  &(uses $n$) & or \\
$SW^{A,C}=SW^A \times SW^C$ &(uses $n^{c=0}$) & 
\end{tabular}\vspace{0.3em}

\noindent \textbf{g-estimation} can only adjust for confounding, not selection bias $\rightarrow$ use IP weights






\paragraph{\large G-Estimation} (additive) structural nested models %todo: verbesserung von allem, ich habs noch nicht ganz verstanden
\begin{align*}
\mathrm{logit} \, \mathrm{Pr}\left[A=1|H(\psi^\dagger), L\right] &= \alpha_0 + \alpha_1H(\psi^\dagger) + \alpha_2L \\
H(\psi^\dagger) &= Y - \psi_\dagger A
\end{align*}
find $\psi^\dagger$ which renders $\alpha_1=0$; 95\,\%-CI: all $\psi^\dagger$ for which $p>0.05$
closed-form solution for linear models


\noindent \textbf{derivation:} $H(\psi^\dagger) = Y^{a=0}$
$$\mathrm{logit} \, \mathrm{Pr}\left[A=1|Y^{a=0}, L\right] = \alpha_0 + \alpha_1Y^{a=0} + \alpha_2L$$
$Y^{a=0}$ unknown, but because of exchangeability $\alpha_1$ should be zero
$$Y^{a=0} =Y^a - \psi_1 a$$
equivalent to $Y^{a=0} =Y^{a=1} - \psi_1$, but using no counterfactuals



\noindent \textbf{structural nested mean model}
\begin{align*}
\text{additive: }\,\,\, & \mathrm{E}\left[Y^a-Y^{a=0}|A=a, L\right] &= \beta_1 a \,(+ \beta_2 a L) \\
\text{multiplicative: }\,\,\, & \log \left( \frac{\mathrm{E}\left[Y^a|A=a, L\right]}{\mathrm{E}\left[Y^{a=0}|A=a, L\right]} \right) &= \beta_1 a \,(+ \beta_2 a L)
\end{align*} 
multiplicative is preferred if $Y$ always positive, but does not extend to longitudinal case

\noindent semi-parametric: agnostic about $\beta_0$ and effect of $L$ $\rightarrow$ robust $\uparrow$

\noindent \textbf{no time-varying:} no nesting; model equals marginal structural models with missing $\beta_0, \beta_3$ (unspecified ``no treatment'')

\noindent \textbf{sensitivity analysis:} unmeasured confounding ($\alpha_1 \neq 0$) can be examined: do procedure  for different values of $\alpha_1$ $\rightarrow$ plot $\alpha_1$ vs.\ $\psi^\dagger$ $\rightarrow$ how sensitive is  estimate to unmeasured confounding?

\noindent \textbf{effect modification:} add $V$ in both g-estimation equations %todo


\noindent \textbf{doubly robust estimators} exist%todo


\paragraph{\large IP Weighting} \textit{i}nverse \textit{p}robability of treatment (g-formula)
$$\mathrm{E}\left[Y^{a}\right] = \mathrm{E}\left[\frac{I(A=a)Y}{f\left[A|L\right]}\right]; W^A=\frac{1}{f\left[A|L\right]}; SW^A = \frac{f(A)}{f\left[A|L\right]}$$

\noindent unknowns can be estimated non-parametrically or modeled
\noindent \textbf{pseudo-population:} everyone is treated \& untreated ($L\not\to A$)

\noindent \textbf{FRCISTG} \textit{(fully randomized causally interpreted structured graph)}: probability tree for $L \rightarrow A \rightarrow Y$, can be used to calculate/visualize simulation of values for $A$ 

\noindent \textbf{for discrete $\boldsymbol{A, L}$} $f\left[a|l\right] = \mathrm{Pr}\left[A=a,L=l\right]$

\noindent \textbf{estimators:} Horvitz-Thompson; Hajek (modified version) %todo p.152 

\noindent \textbf{stabilized weights $\boldsymbol{SW^A}$} should have an average of 1 (check!) $\rightarrow$ pseudo-population same size $\rightarrow$ CI width $\downarrow$

\paragraph{\large Standardization and IP Weighting}

are equivalent, \textit{\textbf{but}} if modeled, different ``no misspecification'' assumptions:

standardization: outcome model

IP weighting: treatment model

\noindent \textbf{doubly robust estimators:} reduce model misspecification bias, consistent if either model is correct; \textbf{\textit{e.\,g.:}} \vspace{-0.9em}

1. fit outcome regression with variable $R = \begin{cases} +W^A & \text{if } A{=}1 \\ -W^A & \text{if } A{=}0 \end{cases}$ \vspace{-0.9em}

2. standardize by averaging



\subsubsection{Time-varying A}







\paragraph{\large IP Weighting} 

$$ W^{\bar{A}} = \prod_{k=0}^K \frac{1}{f\left(A_k|\bar{A}_{k-1}, \bar{L}_k\right)}$$


$$ SW^{\bar{A}} = \prod_{k=0}^K \frac{f\left(A_k|\bar{A}_{k-1}\right)}{f\left(A_k|\bar{A}_{k-1}, \bar{L}_k\right)}$$

\paragraph{\large Doubly Robust Estimator} sequential estimation
\begin{enumerate}[leftmargin=*, itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item 
estimate $\hat{f}\left(A_m|\bar{A}_{m-1}, \bar{L}_m\right)$ (e.\,g.\ logistic model), use it to \newline
calculate at each time $m$: $\widehat{W}^{\bar{A}_m} = \prod_{k=0}^m\frac{1}{\hat{f}\left(A_k|\bar{A}_{k-1}, \bar{L}_k\right)}$ and modified IP weights at $m$: $\widehat{W}^{\bar{A}_{m-1, a_m}} = \frac{\widehat{W}^{\bar{A}_{m-1}}}{\hat{f}\left(a_m|\bar{A}_{m-1}, \bar{L}_m\right)} $
\item with $\widehat{T}_{K+1}:=Y$, recursively for $m=K, K-1, ..., 0$:\newline
 (a) fit outcome regression on $\widehat{T}_{m+1}$ with variable  $\widehat{W}^{\bar{A}_m}$\newline
 (b) calculate $\widehat{T}_{m}$ using the outcome model with $\widehat{W}^{\bar{A}_{m-1, a_m}}$
\item calculate standardized mean outcome $\widehat{\mathrm{E}}\left[Y^{\bar{a}}\right] = \mathrm{E}\left[\widehat{T}_0\right]$
\end{enumerate}

\noindent \textbf{valid, if} treatment or outcome model correct, or treatment correct until k and outcome otherwise ($k+1$ robustness)


\paragraph{\large G-Estimation} nested equations: for each time $k$

\noindent \textbf{strutural nested mean models} separate effect of each $a_k$
$$\mathrm{E}\left[Y^{\bar{a}_{k-1}, a_k, \underline{0}_{k+1}} - Y^{\bar{a}_{k-1}, \underline{0}_{k+1}}|\bar{L}^{\bar{a}_{k-1}}=\bar{l}_k, \bar{A}_{k-1} = \bar{a}_{k-1}\right] = $$
$$  a_k\gamma_k\left(\bar{a}_{k-1}, \bar{l}_k,\beta\right)$$
\noindent 

\noindent \textbf{calculations}
$$H_k\left(\psi^\dagger\right) = Y - \sum_{j=k}^K A_j \gamma_j\left(\bar{A}_{j-1}, \bar{L}_j, \psi^\dagger\right)$$
\noindent function $\gamma_j$ can be, e.\,g.\ constant ($\psi_1$), time-varying only ($\psi_1+\psi_2k$), or dependent on treatment/covariate history

$$\mathrm{logit}\,\mathrm{Pr}\left[A_k=1|H_k\left(\psi^\dagger\right), \bar{L}_k,\bar{A}_{k-1}\right]=$$
$$\alpha_0 +\alpha_1H_k\left(\psi^\dagger\right)+\alpha_2 w_k\left(\bar{L}_k,\bar{A}_{k-1}\right)
$$
find $\alpha_1$ that is closest to zero

a closed form estimator exists for the linear case



\paragraph{\large Censoring} $\bar{C}$: monotonic type of missing data 

\noindent \textbf{standardization}: $\int f(y|\bar{a}, \bar{c}{=}\bar{0}, \bar{l}) \prod_{k=0}^K dF\left(l_k|\bar{a}_{k-1}, c_{k-1} {=} 0, \bar{l}_{k-1}\right)$

\noindent \textbf{IP weighting}:
\noindent $$\textcolor{gray}{S}W^{\bar{C}}= \prod_{k=1}^{K+1}\frac{1\textcolor{gray}{\cdot \mathrm{Pr}\left(C_k=0|\bar{A}_{k-1}, C_{k-1}=0\right)}}{\mathrm{Pr}\left(C_k=0|\bar{A}_{k-1}, C_{k-1}=0, \bar{L}_k\right)} $$










\end{multicols}



\subsection{Doubly Robust Methods}

\begin{multicols}{2}

\paragraph{\large  Advantages} \citep{van2011targeted}

\noindent \textbf{consistent} \textit{if either $\bar{Q}_0$ or $g_n$ are consistent (doubly robust)}:
$$\forall \epsilon>0, P \in \mathcal{M}: \mathrm{Pr}_P\left[|\hat{\theta}_n-\theta(P)|>\epsilon\right] \to 0 \text{ as } n\to\infty$$

\noindent \textbf{collaboratively doubly robust:} $g_n$ only needs predictors of $Y$, as it does not try to fit $g_0$ well, but improve the fit of $\bar{Q}^*_n$

\noindent \textbf{asymptotic unbiasedness}
\textit{if either $\bar{Q}_0$ or $g_0$ are consistent}, super learning makes $\bar{Q}_0$ and $g_n$ max.\ asymptotically unbiased

\noindent \textbf{asymptotic efficiency} \textit{if both $\bar{Q}_0$ and $g_n$ are consistent}: achieves Cramer-Rao bound of minimum possible asymptotic variance (requires asymptotic unbiasedness)

\noindent \textbf{asymptotic linearity} \textit{if either  $\bar{Q}_0$ or $g_n$ are consistent}: \newline means estimator behaves like empirical mean
\begin{itemize}[leftmargin=*, itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item bias converges to zero at rate smaller than $1/\sqrt{n}$
\item for large $n$ estimator is approximately normally distributed
\end{itemize}

\paragraph{\large Influence Curve} \citep{hampel1974influence, van2011targeted}
how robust is estimator toward extreme values?
$$IC_{T,P_n}(O)=\lim_{\epsilon\to 0}\frac{T\left[\left(1-\epsilon\right)P_n + \epsilon\delta_O\right]-T(P_n)}{\epsilon}$$
for estimator $T$ and distribution $P_n$, with $0<\epsilon<1$ and $\delta_O$ probability measure determined by the point mass 1

%what is delta_x / delta_omega
\noindent can also be rewritten as
$$IC_{T,P_n}=\frac{d}{d\epsilon}T\left[\left(1-\epsilon\right)P_n + \epsilon\delta_O\right] = 
\frac{d}{dP_n}T\left(\delta_O - P_n\right)$$
last part is a \textit{\textbf{directional derivative}} at $P_n$ (empirical probability measure that puts mass $1/n$ on $O_i$) in direction $(\delta_O - P_n)$




\noindent $\boldsymbol{\overline{IC}(P_0)} = 0$ and $\mathrm{Var}(IC(P_0))$ is the asymptotic variance of the standard estimator  $\sqrt{n} (\psi_n - \psi_0)$, therefore $Var(\hat{\Psi}(P_n)) = \frac{Var_{IC}}{n}$

\noindent \textbf{efficient IC:} an estimator is asymptotically efficient $\Leftrightarrow$ its influence curve is the efficient influence curve $IC(O)=D^*(O)$


%todo Hampel paper



\paragraph{\large TMLE} \citep{van2011targeted}

\textit{t}argeted \textit{m}aximum \textit{l}ikelihood \textit{e}stimation
$$O=(W, A, Y) \sim P_0$$

\noindent target $\Psi(P_0) = \Psi(\bar{Q}_0, Q_{W,0}) = \psi_0$, 

\textit{often: $\mathrm{E}_{W,0}\left[\mathrm{E}_0(Y|A{=}1,W) {-} \mathrm{E}_0(Y|A{=}0,W)\right]$}

\noindent \textbf{first step:} outcome model $\bar{Q}^0_n(A,W)$ estimating $\bar{Q}_0$ (part of $P_0$)
\begin{itemize}[leftmargin=*, itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item  super learning is often used here, but leads to a biased estimate
\item not all of $f(Y|A,W)$ needs to be estimated, just the relevant portion, \textit{typically average  outcome $\mathrm{E}_0(Y|A,W)$}  $\rightarrow$ efficiency $\uparrow$
\end{itemize}

\noindent \textbf{second step:} update $\bar{Q}^0_n(A,W)$ to $\bar{Q}^1_n(A,W)$ using treatment model $g_n$ estimating $g_0 = P_0(A|W)$ 
\begin{enumerate}[leftmargin=*, itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item model $g_n$,  super learning is a popular choice here, too
\item calculate $n$ clever covariates: $H^*_n(A,W) {=} \begin{cases}\frac{1}{g_n(1|W)}  &\text{if } A_i{=}1 \\   \frac{1}{g_n(0|W)}        &\text{if } A_i{=}0 \end{cases}$
\item update $\bar{Q}_n^0$, by estimating $\epsilon_n$ with offset logistic regression: $\mathrm{logit} \bar{Q}_n^1(A,W) = \mathrm{logit} \bar{Q}_n^0(A,W) + \epsilon_n H_n^*(A,W)$ \newline (converges after first update), then calculate counterfactuals
\end{enumerate}
\begin{itemize}[leftmargin=*, itemsep=0em, topsep=0pt, partopsep=0pt,parsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\item goal: bias reduction, get optimal bias-variance trade-off
\item removes all asymptotic bias, if consistent estimator is used here
\end{itemize}

\noindent \textbf{third step:} use empirical distribution for $Q_{W,0}$ in a substitution estimator, \textit{e.\,g.}: $\psi_n^{TMLE} = \frac{1}{n}\sum_{i=1}^n \left[\bar{Q}^1_n(1,W_i) -  \bar{Q}^1_n(0,W_i)\right] $ 



\noindent \textbf{advantages:} loss-based (does not only solve efficient influence curve estimating equation, but also uses a loss and working model preserving global constraints), well-defined (as a loss-based learner), substition estimator (respects global constraints $\to$ more robust to outliers and sparsity)



\noindent \textbf{closed form inference based on the influence curve:}
\begin{align*}IC_n^*(O_i) &= \overbrace{\left[\frac{\mathbbm{1}(A_i=1)}{g_n(1,W_i)} - \frac{\mathbbm{1}(A_i=0)}{g_n(0,W_i)}\right] \left[Y-\bar{Q}^1_n(A_i,W_i)\right]}^{a} \\ 
&+ \overbrace{\bar{Q}^1_n(1,W_i) - \bar{Q}^1_n(0,W_i) -\psi_{TMLE,n}}^{b} 
\end{align*}
TMLE sets the mean of the IC, $\overline{IC}_n$, to zero ($b$ has already mean zero, see third step, the first part of $a$ is the clever covariate)

\noindent \textit{sample variance} is then: $S^2(IC_n) = \frac{1}{n}\sum_{i=1}^n\left(IC_n(o_i) - \bar{IC}_n\right)^2$

\noindent \textit{standard error} of estimator: $\sigma_n = \sqrt{\frac{S^2(IC_n)}{n}}$

\noindent \textit{95\% CI:} $\psi_{TMLE,n} \pm z_{0.975}\frac{\sigma_n}{\sqrt{n}}$; p-value: $2\left[1-\Phi\left(\left|\frac{\psi_{TMLE,n}}{\sigma_n/\sqrt{n}}\right|\right)\right]$


\paragraph{\large LTMLE} longitudinal

\noindent for $t=T,..., 1$:

1. model $\mathrm{E}(Y_t|\bar{A}_{t-1}, \bar{L}_t)$ (fit on individuals that are uncensored and alive at $t-1$

2. plug in $\bar{a}_{t-1}=\bar{d}_{t-1}$; use regression from 1 to predict outcome at time $t$, ie. $\bar{Y}_t^{\bar{d}_t}$ 

3. update estimate
with $Y_{t} = offset(step 2 result in t) + \epsilon \times clever covariate$: predict $\bar{Y}_t^{\bar{d}_t}$ (alternatively the clever covariate can be used as a weight)

4. $\hat{\psi}_T =$ mean of $\bar{Y}_1^{\bar{d}_1}$






\paragraph{\large TMLE advanced} \citep{van2011targeted}

\textit{t}argeted \textit{m}inimum \textit{l}oss-based \textit{e}stimation


\noindent target parameter $\Psi: \mathcal{M} \to \mathbb{R}$, with $\mathcal{M}$ the statistical model used

1. compute its pathwise derivative at $P$ and corresponding canonical gradient $D^*(P)$ (efficient influence curve: a function of $O$ with mean zero under $P$)

2. define loss function $L()$ s.t. $P\to E_0L(P)$ is minimized at true $P_0$ (or just relevant $Q$)

3. for a $P$ in model $\mathcal{M}$ define a parametric working model $\left\{P(\epsilon):\epsilon\right\}$ s.t. $P(\epsilon=0)=P$ and a ``score'' $\frac{d}{d\epsilon}L(P(\epsilon))$:  score (or linear combination of its components) equals $D^*(P)$ at $P$ (or just relevant $Q$)

4. with initial estimate $P_n^0$, compute $\epsilon_n^0=\arg\min_\epsilon \sum_{i=1}^n L(P_n^0(\epsilon))(O_i)$, calculate first iteration $P^1_n=P_n^0(\epsilon_n^0)$, repeat until $\epsilon^k_n=0$ (or just relevant $Q$)

5. get TMLE estimate $\psi_0$ as the substitution estimator pluggint $P^*_n$ into $\Psi$

6. TMLE olves the efficient influence curve equation $0=\sum_{i=1}^n D^*(P^*_n)(O_i)$ $\to$ esymptotic linearity and efficiency

%todo read chapter 5


\ \\
\noindent----------------------------------------------------------------------------------



$\mathcal{L}(O) = \overbrace{\mathrm{Pr}(Y|A, W)}^{Q_Y}\overbrace{\mathrm{Pr}(A|W)}^{g}\overbrace{\mathrm{Pr}(W)}^{Q_W}$



$H(A,W)$ depends on target parameter and loss function but is a function of the propensitiy score
update initial fit $\bar{Q}^*_n = \bar{Q}^0_n + \hat{\epsilon}H(A,W)$


valid inference, good finite sample performance, 


$H(A,W)$ comes from the influence curve, targeting ensures mean of efficient influence curve $D^*(P)$  is zero

TMLE solves $P_nD^*(P^*_n)=0$



TMLE is a substitution estimator $\psi_n^{TMLE} = \frac{1}{2}\sum_{i=1}^n \bar{Q}^*_n(1,W_i) - \frac{1}{2}\sum_{i=1}^n \bar{Q}^*_n(0,W_i)$ therefore mean of b is zero

targeting step makes sure a also has mean zero



MLE solves $\sum_{i=1}^n H(A_i, W_i)\left[Y_i-\bar{Q}^*_n(A_i,W_i)\right]=0$ where $\bar{Q}^*_n(A_i,W_i) = \hat{\epsilon}H(A,W) + \bar{Q}^0_n$
 therefore obvious choice: $H(A,W) = \frac{A}{g(1,W)} - \frac{1-A}{g(0,W)}$
 
 
 influence curve based inference: 
 asymptotic linearity $\sqrt{n}\left(\psi_n^{TMLE} - \psi_0\right) \overset{D}{\rightarrow} \mathrm{N}(0,\sigma^2)$
 
 \paragraph{\large AIPTW} \textit{a}ugmented \textit{i}nverse \textit{p}robability of \textit{t}reatment \textit{w}eighting
 
 %todo pages 105-106
 
 disadvantages \citep{van2011targeted}: ignores global constraints $\to$ often unstable under sparsity, sometimes not well-defined
 


\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: LONGITUDINAL

%-------------------------------------------------------------------------------

\section{Longitudinal Data}
\begin{multicols}{2}

\paragraph{\large Time-Varying Treatments} compare 2 treatments

\noindent treatment history up to $k$: $\bar{A}_k=(A_0, A_1, ..., A_k)$

\noindent shorthand: always treated $\bar{A} = \bar{1}$, never treated $\bar{A} = \left(\bar{0}\right)$

\textbf{static strategy:} $g=\left[g_0(\bar{a}_{-1}), ..., g_K(\bar{a}_{K-1})\right]$

\textbf{dynamic strategy:} $g=\left[g_0(\bar{l}_0), ..., g_K(\bar{l}_K)\right]$

\textbf{stochastic strategy:} non-deterministic $g$

\noindent optimal strategy is where $\mathrm{E}\left[Y^g\right]$ is maximized (if high is good)






\paragraph{\large Sequential Identifiability} sequential versions of

 \textbf{exchangability:}
$Y^g \indep A_k| \bar{A}_{k-1} \,\,\, \forall g, k=0,1,...,K$

\textit{conditional exchangeability:}
$$\left(Y^g, L^g_{k+1}\right) \indep A_k| \bar{A}_{k-1} {=} g\left(\bar{L}_k\right), \bar{L}^k \,\,\, \forall g, k=0,1,...,K$$

 \textbf{positivity:} $f_{\bar{A}_{k-1},\bar{L}_k}(\bar{a}_{k-1},\bar{l}_k)\neq 0 \,\, \Rightarrow$
$$ f_{A_k|\bar{A}_{k-1},\bar{L}_k}(a_k|\bar{a}_{k-1},\bar{l}_k)>0 \,\, \forall \left(\bar{a}_{k-1},\bar{l}_k\right)$$

 \textbf{consistency:} 
\begin{align*}
Y^{\bar{a}} = Y^{\bar{a}^*} & \, \text{ if } {\bar{a}} = {\bar{a}^*};  & \,
Y^{\bar{a}} = Y & \, \text{ if } {\bar{A}} = {\bar{a}};  \\
\bar{L}^{\bar{a}}_k = \bar{L}^{\bar{a}^*}_k & \, \text{ if } {\bar{a}_{k-1}} = {\bar{a}^*_{k-1}}; & \,
\bar{L}^{\bar{a}}_k = \bar{L}_k & \, \text{ if } {\bar{A}_{k-1}} = {\bar{a}_{k-1}}
\end{align*}


\noindent \textbf{generalized backdoor criterion} (static strategy):\,all backdoors into $A_k$ (except through future treatments) are blocked $\forall k$ 


\noindent \textbf{static sequential exchangeability for $\boldsymbol{Y^{\bar{a}}}$}
$$Y^{\bar{a}} \indep A_k| \bar{A}_{k-1}, \bar{L}_k \,\,\,\,\, \text{ for } k=0,1,...,K$$
use SWIGs to visually check d-separation

\noindent \textbf{time-varying confounding} $\mathrm{E}\left[Y^{\bar{a}}|L_0\right] \neq \mathrm{E}\left[Y|A=\bar{a}, L_0\right]$


\paragraph{\large Treatment-Confounder Feedback} $A_0 \rightarrow L_1 \rightarrow A_1$: an unmeasured $U$ influencing $L_1$ and $Y$ turns $L_1$ into a collider;

\noindent traditional adjustment (e.\,g.\ stratification) biased: use g-methods

\noindent \textbf{g-null test} sequential exchangeability \& sharp null true $\Rightarrow$ $Y^g = Y \,\, \forall g$ $\,\,\,\,\Rightarrow \,\,\,\,$ $Y \indep A_0|L_0$ \& $Y \indep A_1|A_0, L_0, L_1$;
therefore: \newline if last two independences don't hold, one assumption is violated

\noindent \textbf{g-null theorem:} $\mathrm{E}\left[Y^g\right] = \mathrm{E}\left[Y\right]$, if the two independences hold \newline($\Rightarrow$ sharp null: only if strong faithfulness (no effect cancelling))



\end{multicols}




\def\bibpreamble{\textit{If no citation is given, the information is taken from the book \citep{hernan2020causal}} \vspace{1.5em}}

\bibliographystyle{apalike} 
\bibliography{cite} 





\end{document}